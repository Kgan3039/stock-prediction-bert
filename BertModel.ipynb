{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kgan3039/stock-prediction-bert/blob/main/BertModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSPU9_F49tfX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets scikit-learn torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nQXNGxXDFqO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#disable WAndB\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "# List all files in the current working directory\n",
        "print(os.listdir())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIQPJNFdDMq1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(os.getcwd())\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdjtTzS3-B6r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Reload the train dataset without headers, skipping the first row\n",
        "train_df = pd.read_csv('stock_data1.csv', header=0)\n",
        "\n",
        "# Rename columns to reflect their content correctly\n",
        "train_df.columns = ['text', 'label']\n",
        "\n",
        "# Drop any row that still contains the column names if it exists\n",
        "train_df = train_df[train_df['label'] != 'Sentiment']\n",
        "\n",
        "# Ensure the label column is numeric\n",
        "train_df['label'] = pd.to_numeric(train_df['label'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN labels after conversion\n",
        "train_df = train_df.dropna(subset=['label']).astype({'label': 'int'})\n",
        "\n",
        "# Load the validation dataset and rename columns appropriately\n",
        "val_df = pd.read_csv('twitter_validation.csv', header=None)\n",
        "val_df = val_df.rename(columns={val_df.columns[2]: 'label', val_df.columns[3]: 'text'})\n",
        "\n",
        "# Drop unnecessary columns (0 and 1)\n",
        "val_df = val_df[['text', 'label']]\n",
        "\n",
        "# Apply label mapping to convert string labels to numeric labels\n",
        "label_mapping = {\n",
        "    'Neutral': 0,\n",
        "    'Positive': 1,\n",
        "    'Negative': -1,\n",
        "    'Irrelevant': 2\n",
        "}\n",
        "val_df['label'] = val_df['label'].replace(label_mapping)\n",
        "\n",
        "# Ensure the label column is numeric\n",
        "val_df['label'] = pd.to_numeric(val_df['label'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN labels after conversion\n",
        "val_df = val_df.dropna(subset=['label']).astype({'label': 'int'})\n",
        "\n",
        "# Check the size of the filtered dataset\n",
        "print(\"Filtered train_df size:\", len(train_df))\n",
        "print(\"Filtered val_df size:\", len(val_df))\n",
        "\n",
        "# Ensure dataset is not empty before conversion\n",
        "if len(train_df) == 0 or len(val_df) == 0:\n",
        "    raise ValueError(\"Filtered dataset is still empty. Please double-check your dataset and filtering criteria.\")\n",
        "\n",
        "# Remap labels from {-1, 1} to {0, 1}\n",
        "train_df['label'] = train_df['label'].replace({-1: 0, 1: 1})\n",
        "val_df['label'] = val_df['label'].replace({-1: 0, 1: 1})\n",
        "\n",
        "# Filter out any unexpected labels that aren't 0 or 1\n",
        "train_df = train_df[train_df['label'].isin([0, 1])]\n",
        "val_df = val_df[val_df['label'].isin([0, 1])]\n",
        "\n",
        "# Verify the label distribution\n",
        "print(\"Train label distribution after remapping:\\n\", train_df['label'].value_counts())\n",
        "print(\"Validation label distribution after remapping:\\n\", val_df['label'].value_counts())\n",
        "\n",
        "# Convert to HuggingFace dataset\n",
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "# Print an example to verify dataset conversion\n",
        "if len(train_dataset) > 0:\n",
        "    print(\"First item in train_dataset:\", train_dataset[0])\n",
        "else:\n",
        "    raise ValueError(\"train_dataset is empty after conversion.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omGMqqHO-GO4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Load tokenizer\n",
        "model_ckpt = \"bert-base-uncased\"  # Specify model checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "   return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)  # Increase max_length if needed\n",
        "\n",
        "# Tokenize the training and validation datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Verify the label set\n",
        "print(\"Unique labels in train_dataset:\", set(train_dataset['label']))\n",
        "print(\"Unique labels in val_dataset:\", set(val_dataset['label']))\n",
        "\n",
        "# Set the number of labels\n",
        "num_labels = 2\n",
        "\n",
        "# Load pre-trained BERT model and adjust for sequence classification\n",
        "try:\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)\n",
        "except Exception as e:\n",
        "    print(\"Error during model loading:\", e)\n",
        "    raise\n",
        "\n",
        "#Freeze base model layers\n",
        "for name, param in model.named_parameters():\n",
        "    if \"layer.10\" in name or \"layer.11\" in name:  # Unfreeze last two layers\n",
        "        param.requires_grad = True\n",
        "    elif \"classifier\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zDdn3bp-INZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    precision = precision_score(labels, predictions, average='weighted')\n",
        "    recall = recall_score(labels, predictions, average='weighted')\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CinSDxkl-M1Y"
      },
      "outputs": [],
      "source": [
        "# Ensure labels are numeric\n",
        "num_labels = len(set(train_dataset['label']))\n",
        "\n",
        "# Model definition\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device)\n",
        "\n",
        "# Freezing base model parameters to fine-tune classifier only\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-6,  # Reduce learning rate\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=15,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK9PmBuD-O7S"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import Trainer\n",
        "import torch\n",
        "\n",
        "# Subclassing Trainer to override compute_loss with weighted cross-entropy\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # Apply weighted cross-entropy loss\n",
        "        class_weights = torch.tensor([0.5, 1.5]).to(self.args.device)  # Adjust weights accordingly\n",
        "        loss_fn = CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Load model before initializing the Trainer\n",
        "num_labels = len(set(train_dataset['label']))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels)\n",
        "model = model.to(device)\n",
        "\n",
        "# Ensure base model layers are frozen (optional)\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Initialize the custom Trainer with the overridden compute_loss function\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Print label information and start training\n",
        "print(\"Number of labels:\", num_labels)\n",
        "print(\"Label set in train_dataset:\", set(train_dataset['label']))\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "except RuntimeError as e:\n",
        "    print(\"RuntimeError during training:\", e)\n",
        "    print(\"Consider using CUDA_LAUNCH_BLOCKING=1 for better debug messages.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(f\"Layer: {name} | Requires Grad: {param.requires_grad}\")"
      ],
      "metadata": {
        "id": "wFFjLIYZWzdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fu59SEcaW5rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f3jwbPX-Vcc"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38Wty3o2-Xp5"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('./finetuned-bert-model')\n",
        "tokenizer.save_pretrained('./finetuned-bert-tokenizer')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_predictions = []"
      ],
      "metadata": {
        "id": "EQlHsQRKZeWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "kJEg9oGvZmSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker = \"TSLA\"\n",
        "start_date = \"2023-01-01\"\n",
        "end_date = \"2023-05-01\"\n",
        "stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "stock_data = stock_data[['Adj Close']].reset_index()\n",
        "stock_data.rename(columns={'Adj Close': 'adj_close'}, inplace=True)\n"
      ],
      "metadata": {
        "id": "jA-9Tib6ZviC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_weighted_sentiment(df):\n",
        "    df['normalized_retweets'] = df['retweets'] / df['retweets'].max()\n",
        "    df['weighted_sentiment'] = df['normalized_retweets'] * df['sentiment']\n",
        "    daily_sentiment = df.groupby('date')['weighted_sentiment'].sum().reset_index()\n",
        "    return daily_sentiment"
      ],
      "metadata": {
        "id": "PIbXxj8fZxwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `sentiment_predictions` is a list of dictionaries, you need to convert it to a DataFrame\n",
        "# Example:\n",
        "# sentiment_predictions = [{'date': '2023-01-01', 'retweets': 10, 'sentiment': 1}, ...]\n",
        "\n",
        "# Convert to DataFrame if it's not already one\n",
        "if isinstance(sentiment_predictions, list):\n",
        "    sentiment_predictions = pd.DataFrame(sentiment_predictions)\n",
        "\n",
        "# Check if `sentiment_predictions` is now a DataFrame\n",
        "print(sentiment_predictions.head())\n",
        "\n",
        "# Function to calculate weighted sentiment\n",
        "def calculate_weighted_sentiment(df):\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise ValueError(\"Input data must be a pandas DataFrame\")\n",
        "\n",
        "    # Ensure required columns are present\n",
        "    if 'retweets' not in df.columns or 'sentiment' not in df.columns or 'date' not in df.columns:\n",
        "        raise ValueError(\"Data must include 'date', 'retweets', and 'sentiment' columns.\")\n",
        "\n",
        "    # Normalize retweet counts\n",
        "    df['normalized_retweets'] = df['retweets'] / df['retweets'].max()\n",
        "\n",
        "    # Weighted sentiment score = normalized retweets * sentiment\n",
        "    df['weighted_sentiment'] = df['normalized_retweets'] * df['sentiment']\n",
        "\n",
        "    # Aggregate by date to get the daily weighted sentiment\n",
        "    daily_sentiment = df.groupby('date')['weighted_sentiment'].sum().reset_index()\n",
        "    return daily_sentiment\n",
        "\n",
        "daily_sentiment = calculate_weighted_sentiment(sentiment_predictions)\n",
        "\n",
        "merged_data = pd.merge(stock_data, daily_sentiment, left_on='Date', right_on='date', how='inner')\n",
        "merged_data = merged_data[['Date', 'adj_close', 'weighted_sentiment']]"
      ],
      "metadata": {
        "id": "7hlcGYMZZ0WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookback_window = 5\n",
        "for i in range(1, lookback_window + 1):\n",
        "    merged_data[f'adj_close_lag_{i}'] = merged_data['adj_close'].shift(i)\n",
        "    merged_data[f'weighted_sentiment_lag_{i}'] = merged_data['weighted_sentiment'].shift(i)\n",
        "merged_data = merged_data.dropna()\n",
        "\n",
        "# Define the target: 1 if price increased, 0 if decreased\n",
        "merged_data['price_direction'] = (merged_data['adj_close'].diff().shift(-1) > 0).astype(int)\n",
        "features = [col for col in merged_data.columns if 'lag' in col]\n",
        "X = merged_data[features]\n",
        "y = merged_data['price_direction']"
      ],
      "metadata": {
        "id": "ycxwgaAjZ5Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "tqybFsPPZ8Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Stock Prediction Accuracy: {accuracy:.2f}, F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "id": "FTw_c49vZ9do"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}